{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dq6nJ4JIpaxE"
   },
   "source": [
    "# RemBG â€” Naive Video Background Removal Pipeline\n",
    "\n",
    "## What this notebook demonstrates\n",
    "\n",
    "This notebook removes the background from every frame of a video using **rembg**, a library built on top of neural segmentation models.\n",
    "\n",
    "### How background removal works\n",
    "\n",
    "Traditional background removal (like green screens) relies on a known, uniform colour.  \n",
    "AI-based removal works differently: a neural network looks at the whole image and predicts which pixels belong to the *foreground subject* (a person, object, etc.) and which belong to the *background*. This is called **salient object detection**.\n",
    "\n",
    "The default model used here is **U2Net** (Qin et al., 2020). It was trained on large datasets of images with annotated foregrounds and learns to produce a soft *alpha mask* â€” a grayscale image where white = keep and black = remove. The mask is then applied to the original frame to produce a transparent PNG.\n",
    "\n",
    "### Why \"naive\"?\n",
    "\n",
    "Each frame is processed **independently** â€” the model has no memory of the previous frame. This means that even tiny differences in lighting or subject position cause the mask to shift slightly between frames, producing visible **flickering or jitter** in the output video.\n",
    "\n",
    "This is an intentional limitation of this demonstrator. Fixing it requires **temporal consistency** techniques (e.g. optical flow, cross-frame attention) explored in other notebooks.\n",
    "\n",
    "**Steps in this notebook:**\n",
    "1. Install dependencies\n",
    "2. Create working folders\n",
    "3. Upload a video\n",
    "4. Extract frames with FFmpeg\n",
    "5. Apply RemBG to each frame individually\n",
    "6. Reassemble frames into a video\n",
    "7. Observe the flickering artefact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 â€” Install dependencies\n",
    "\n",
    "| Package | Role |\n",
    "|---|---|\n",
    "| `rembg` | Background removal library. Wraps the U2Net model and exposes a simple `remove()` function. |\n",
    "| `ffmpeg-python` | Python bindings for FFmpeg. Used to extract frames from a video and reassemble them afterwards. |\n",
    "| `onnxruntime` | Runtime engine for ONNX models. U2Net is distributed as an `.onnx` file, which `rembg` downloads automatically on first use. ONNX (Open Neural Network Exchange) is a portable model format that runs on CPU or GPU without requiring a specific training framework like PyTorch. |\n",
    "\n",
    "> **GPU note:** By default `onnxruntime` runs on CPU. Installing `onnxruntime-gpu` instead enables CUDA acceleration and speeds up processing significantly on longer videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8351,
     "status": "ok",
     "timestamp": 1744204209371,
     "user": {
      "displayName": "Eduardo",
      "userId": "02969445285498016452"
     },
     "user_tz": -120
    },
    "id": "I8ZSKEcrpaxF",
    "outputId": "4367d527-76ac-4829-83c1-6b89ca42b67a"
   },
   "outputs": [],
   "source": [
    "# ðŸ“¦ Install dependencies\n",
    "%pip install rembg ffmpeg-python onnxruntime\n",
    "%pip install rembg ffmpeg-python\n",
    "%apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 â€” Create working folders\n",
    "\n",
    "We work with individual image files rather than the video directly, so we need two staging folders:\n",
    "\n",
    "- `frames/` â€” raw frames extracted from the input video\n",
    "- `output_frames/` â€” frames after background removal\n",
    "\n",
    "Both are temporary; they are not the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgWxq-v9paxG"
   },
   "outputs": [],
   "source": [
    "# ðŸ“ Create folders\n",
    "import os\n",
    "for folder in [\"frames\", \"output_frames\"]:\n",
    "    os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 â€” Upload your video\n",
    "\n",
    "This cell uses the Colab file uploader. Upload an MP4 or MOV clip â€” **shorter clips (5â€“15 seconds) are recommended** to keep processing time reasonable on CPU.\n",
    "\n",
    "**Tips for best results:**\n",
    "- The subject should be clearly separated from the background (e.g. a person against a wall).\n",
    "- Avoid very busy or textured backgrounds â€” U2Net can struggle with complex scenes.\n",
    "- Consistent lighting across the clip reduces mask instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 28881,
     "status": "ok",
     "timestamp": 1744204069762,
     "user": {
      "displayName": "Eduardo",
      "userId": "02969445285498016452"
     },
     "user_tz": -120
    },
    "id": "xTGNsO03paxG",
    "outputId": "5a2e61bd-3c91-4149-93b6-dbe79b6ce843"
   },
   "outputs": [],
   "source": [
    "# â¬†ï¸ Upload your video\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 â€” Extract frames from the video\n",
    "\n",
    "A video is just a sequence of still images (frames) displayed in rapid succession (typically 24â€“60 frames per second). To process it with a per-image model we must:\n",
    "\n",
    "1. **Decode** the video into individual frames â€” FFmpeg reads the compressed video stream and outputs one PNG file per frame.\n",
    "2. **Process** each frame.\n",
    "3. **Re-encode** the processed frames back into a video.\n",
    "\n",
    "`qscale=2` sets FFmpeg's JPEG-like quality scale for PNG export. Lower values = higher quality (range 1â€“31). We use 2 to preserve fine detail in hair and edges that the model needs to segment accurately.\n",
    "\n",
    "> The frame filenames (`frame_00001.png`, `frame_00002.png`, â€¦) encode the order. This ordering is essential when reassembling the video later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7018,
     "status": "ok",
     "timestamp": 1744204080641,
     "user": {
      "displayName": "Eduardo",
      "userId": "02969445285498016452"
     },
     "user_tz": -120
    },
    "id": "vflwTeskpaxG",
    "outputId": "00c0fac2-b68f-46db-a0b2-2485018b9dfa"
   },
   "outputs": [],
   "source": [
    "# ðŸŽžï¸ Extract frames from the video\n",
    "import ffmpeg\n",
    "\n",
    "input_video = list(uploaded.keys())[0]\n",
    "(\n",
    "    ffmpeg\n",
    "    .input(input_video)\n",
    "    .output('frames/frame_%05d.png', qscale=2)\n",
    "    .run()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 â€” Apply background removal frame by frame\n",
    "\n",
    "`rembg.remove()` takes raw image bytes and returns a PNG with an **alpha channel** added. The alpha channel is a fourth pixel channel (RGBA) that controls transparency:\n",
    "- **255 (white)** = fully opaque â€” this pixel belongs to the foreground subject.\n",
    "- **0 (black)** = fully transparent â€” this pixel is background and should be discarded.\n",
    "- Values in between produce soft, semi-transparent edges (important for hair and fur).\n",
    "\n",
    "Internally, `remove()`:\n",
    "1. Resizes the image to the model's input resolution.\n",
    "2. Runs a forward pass through U2Net, producing a probability map (the \"saliency map\").\n",
    "3. Post-processes the map into a binary-ish mask with soft edges.\n",
    "4. Applies the mask to the original image as an alpha channel.\n",
    "\n",
    "On first run the model weights (`u2net.onnx`, ~176 MB) are downloaded and cached in `~/.u2net/`. Subsequent runs use the cache.\n",
    "\n",
    "> **Performance note:** Each frame is an independent model call. For a 30 fps, 10-second clip that is 300 inference passes. This is why GPU acceleration matters for real-time or near-real-time use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383950,
     "status": "ok",
     "timestamp": 1744204601616,
     "user": {
      "displayName": "Eduardo",
      "userId": "02969445285498016452"
     },
     "user_tz": -120
    },
    "id": "Y9CoMw71paxG",
    "outputId": "ca22469d-70ec-4ebf-d5af-06851d2e6ae0"
   },
   "outputs": [],
   "source": [
    "# âœ‚ï¸ Apply RemBG frame-by-frame\n",
    "from rembg import remove\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "input_dir = \"frames\"\n",
    "output_dir = \"output_frames\"\n",
    "\n",
    "for filename in sorted(os.listdir(input_dir)):\n",
    "    if filename.endswith(\".png\"):\n",
    "        with open(os.path.join(input_dir, filename), \"rb\") as inp:\n",
    "            input_data = inp.read()\n",
    "            output_data = remove(input_data)\n",
    "        with open(os.path.join(output_dir, filename), \"wb\") as out:\n",
    "            out.write(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” Inspect a single frame â€” before and after background removal\n",
    "# This cell visualises the alpha mask alongside the original and processed frame.\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "sample_file = sorted(os.listdir(\"frames\"))[0]\n",
    "\n",
    "original = Image.open(f\"frames/{sample_file}\").convert(\"RGB\")\n",
    "processed = Image.open(f\"output_frames/{sample_file}\").convert(\"RGBA\")\n",
    "\n",
    "# The alpha channel is the mask predicted by U2Net\n",
    "alpha_mask = processed.split()[-1]  # Extract alpha channel as greyscale image\n",
    "\n",
    "# Composite the processed frame over a white background so transparency is visible\n",
    "white_bg = Image.new(\"RGB\", processed.size, (255, 255, 255))\n",
    "white_bg.paste(processed, mask=alpha_mask)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(original);       axes[0].set_title(\"Original frame\")\n",
    "axes[1].imshow(alpha_mask, cmap=\"gray\"); axes[1].set_title(\"Alpha mask (U2Net output)\")\n",
    "axes[2].imshow(white_bg);       axes[2].set_title(\"Result (composited on white)\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(f\"Frame: {sample_file}\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 â€” Reassemble the video\n",
    "\n",
    "FFmpeg reads the numbered PNGs in order and encodes them into an MP4:\n",
    "\n",
    "- **`vcodec=libx264`** â€” H.264, the most widely compatible video codec. Works in browsers, phones, and every video player.\n",
    "- **`pix_fmt=yuv420p`** â€” the pixel format expected by H.264. It does **not** support an alpha channel, so the transparency from rembg is composited over **black** in the output. To preserve transparency you would use `libvpx-vp9` with `yuva420p` and output a `.webm` file.\n",
    "- **`framerate=30`** â€” must match the framerate used during extraction, otherwise the video plays at the wrong speed.\n",
    "\n",
    "> **Watch for flickering.** Play the output video and notice how the mask boundary shifts between frames. This is the core artefact of frame-independent processing. Compare it to your input and ask yourself: what information from adjacent frames could the model use to stabilise the mask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "resources": {
      "http://localhost:8080/output.mp4": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      }
     }
    },
    "executionInfo": {
     "elapsed": 4229,
     "status": "ok",
     "timestamp": 1744204659518,
     "user": {
      "displayName": "Eduardo",
      "userId": "02969445285498016452"
     },
     "user_tz": -120
    },
    "id": "q3b-lStGpaxH",
    "outputId": "668bf724-2bae-48cc-9575-15a292c5532c"
   },
   "outputs": [],
   "source": [
    "# ðŸ§± Rebuild video from processed frames\n",
    "output_name = \"output.mp4\"\n",
    "(\n",
    "    ffmpeg\n",
    "    .input('output_frames/frame_%05d.png', framerate=30)\n",
    "    .output(output_name, vcodec='libx264', pix_fmt='yuv420p')\n",
    "    .run()\n",
    ")\n",
    "from IPython.display import Video\n",
    "Video(output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection â€” What could be improved?\n",
    "\n",
    "| Limitation | Possible solution |\n",
    "|---|---|\n",
    "| Flickering (no temporal consistency) | Propagate the previous frame's mask via optical flow to constrain the current prediction |\n",
    "| Slow per-frame inference | Batch frames through the model; use GPU via `onnxruntime-gpu` |\n",
    "| Black background in output | Export as WebM with alpha channel, or composite over a custom background |\n",
    "| U2Net struggles with complex edges | Try `birefnet-general` (a newer, higher-quality model available in rembg) |\n",
    "| Hard mask edges | `rembg` supports matting post-processing (`om=True`) for softer transitions |\n",
    "\n",
    "These are the problems that production tools like DaVinci Resolve, Adobe After Effects, and cloud APIs solve â€” each with their own trade-offs between speed, quality, and cost."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
